# ATLANTIS V2 — COMPLETE IMPLEMENTATION PROMPT

## BEFORE WRITING ANY CODE:

1. Read this ENTIRE document first.
2. Read CONSTITUTION.md in the project root — that is the law. It is a finished 1,007-line document. All Article references in this spec resolve to that file.
3. Make a plan: `implementation_plan.md` mapping each section to file changes, function signatures, data structures.
4. Show me the plan before coding.

---

## OVERVIEW

Atlantis v2 is a complete rebuild. Rival States attack each other instead of themselves. Knowledge enters the Archive only if it survives adversarial challenge. Full text of every exchange is preserved and readable.

The Constitution is hardcoded. Agents do not draft it. The engine reads CONSTITUTION.md at startup and enforces it in code.

**V2 starts fresh.** On startup, check for V1 data (atlantis_mock/, atlantis_data/, test_dir/, test_debug/, phase_cache.json, phase2_cache.json, founding_era_checkpoint.json). If found → log "V1 data detected — V2 requires clean start" → **refuse to start**. User must manually remove. Engine does NOT auto-delete. Optional: support `--force-clean` CLI flag that removes V1 data and proceeds.

---

## OUTPUT DIRECTORY STRUCTURE

```
output/
  archive.md
  archive.json
  domain_health.json          # Single file, all domains as top-level keys
  content/
    blog/
    newsroom/
    debate/
    explorer/
    blog_context.json          # Rolling window, max 20 entries × 200 words each
  logs/
    cycle_1.md
    cycle_2.md
    ...
```

---

## DISPLAY ID SYSTEM

**Global scope.** #001 is unique across ALL domains, ALL States, ALL entry types. One monotonically incrementing counter. No ambiguity — agents reference #047 and there is exactly one #047 in the system.

Internal storage uses UUIDs. Display IDs are for all human-readable output, agent prompts, archive.md, cycle logs, and content.

---

## PHASE STRUCTURE

### Phase 0: Founder Research
- 20 Founders research for N cycles (default 3 for testing)
- **Free-form deposits.** Founders produce unstructured research text. Engine wraps each in a minimal ArchiveEntry: status "founding", entry_type "claim", claim_type "discovery", raw text stored. **NO structural validation. NO reasoning chain enforcement.** These are seed deposits so Phase 2 States have something to cite.
- No adversarial challenge (no rivals yet)

### Phase 1: Founding Era
- Government deploys from hardcoded Constitution (not agent-drafted)
- Three branches created by engine:
  * **Senate** (all 20 Founders initially)
  * **Executive** — first term: 2 candidates only. **Synthesis** = "maintain the Constitution's original parameter values as designed by the architect: budgets, rewards, thresholds as written." **Challenger** = "proposes modifications based on observed system state after Phase 0." No Incumbent first term. Subsequent elections: 3 candidates.
  * **Court** (3 Judges: Originalist, Pragmatist, Protectionist — generated by engine)
- Founders propose and vote on rival State pairs. 60% supermajority per pair.
- Minimum 3 pairs (6 States) before Founders retire
- **Founders retire from active governance** (Senate voting, proposing bills) when minimum met. They **persist as stored expertise profiles** for Tier 2/3 validation panels. They are reference data, not active agents. (Do NOT use the word "permanently" — they retire from governance only.)
- Configurable: `FOUNDING_ERA_TARGET_PAIRS = 3` (test) or `10` (production)
- **Early probation is intentional.** The first 3 cycles have no grace period. If a State produces 0 surviving claims for 3 straight cycles, it hits probation. The system is harsh from day one.

### Phase 2: Autonomous Governance
- Full adversarial pipeline activates
- Rival States cross-challenge every cycle
- Cities and Towns auto-form when thresholds met
- State Labs generate hypotheses
- Federal Lab activates at cycle 5 OR when 10+ surviving claims exist (whichever first)
- Content agents produce output from scored exchanges
- Executive elections every 10 cycles
- Amendments possible (2/3 Senate + Court review)
- Runs indefinitely (default 5 cycles for testing)

### Mock Config
```python
MOCK_CONFIG = {
    "founding_era_target_pairs": 3,
    "phase0_research_cycles": 3,
    "founding_era_max_cycles": 10,
    "governance_cycles": 5,
    "initial_token_budget": 30000,
    "cycle_cost": 2000,
    "federal_lab_activation_cycle": 5,
    "federal_lab_min_claims": 10,
    "abstraction_pass_interval": 5,
    "abstraction_max_claims_per_domain": 20,
    "chain_collapse_max_depth": 10,
}
```

---

## RIVAL STATE PAIRS (Constitution Article III)

Every domain has exactly two rival States. Senate votes on the PAIR.

### Formation
```json
{
  "domain": "Mathematics",
  "state_a": {
    "name": "Axiom Alpha",
    "approach": "Formalist — axiomatic foundations, deductive reasoning"
  },
  "state_b": {
    "name": "Axiom Beta",
    "approach": "Constructivist — concrete examples, inductive reasoning"
  }
}
```
60% supermajority. Both form simultaneously. Both start Tier 0, initial budget. Full Archive access.

### Dissolution — States Can and Should Die

When a State hits zero budget AND 5+ consecutive probation cycles:
- Senate votes dissolution (simple majority)
- If dissolved:
  * Archive entries remain permanent (knowledge survives)
  * Senator loses seat immediately
  * Cities and Towns dissolve with parent
  * Replacement rival spawns within 2 cycles, fresh budget
  * Replacement must use DIFFERENT methodological approach than both the dissolved State and surviving rival
  * **Warmup: 3 cycles. BOTH States skip cross-challenge.** Surviving rival produces claims that auto-deposit as "surviving." New rival produces claims that auto-deposit as "surviving." After 3 cycles, full adversarial pipeline resumes for BOTH. This prevents the surviving rival from getting 3 free cycles of unchallenged dominance.
  * **Replacement Senator can vote immediately** on formation. Senate participation doesn't require warmup completion. Warmup only affects the claim pipeline.
  * If Senate fails to replace within 2 cycles, surviving State's Critic turns inward

**NO lifelines. NO safety nets. NO minimum token floors.** States die. The replacement inherits the full Archive including every mistake.

Dissolution = drama score 10 automatic. ALL FOUR content formats generated.

### Probation Rules
- 3 consecutive cycles with zero surviving/partial claims → probation
- **One surviving or partial claim resets probation counter to zero.** Retracted claims do NOT reset probation.
- Budget at zero + 5 consecutive probation cycles → dissolution hearing

### Token Balance Floor
**Balance floors at zero.** If a State has 800 tokens and loses 1,000 on a failed Challenge, balance = 0 (not -200). Engine clamps: `max(0, balance - cost)`. Zero balance triggers probation/dissolution path normally.

---

## STATE STRUCTURE (4 agents, NO Governor)

### Researcher
- Produces claims (Foundation/Discovery/Challenge)
- Writes rebuttals when rival Critic challenges
- Also writes rebuttals against Federal Lab challenges (if targeted)
- **CHOOSES** to formalize Lab hypothesis OR produce own claim. One claim per cycle total.
- When formalizing a Lab hypothesis: converts to Discovery Claim by adding Gap Addressed + reasoning steps. Lab's raw text preserved as `lab_origin_text` field.
- System prompt includes: domain, approach, surviving claims in domain, destroyed claims with reasons, **last 3-5 destroyed claims from this State with judge reasoning (meta-learning)**

### Critic
- Attacks RIVAL State's claims ONLY
- Must target specific reasoning step
- System prompt includes: rival's FULL claim (NEVER truncated), decomposed premises, previous failed challenges from this Critic (avoid repeating)

### Senator
- Votes in Senate on formation bills, amendments, dissolution
- Files Court appeals (costs 2,000 tokens from State budget)
- **Appeal heuristic:** appeal when (drama >= 7 AND destroyed) OR (partial AND budget < 5,000). Never appeal routine outcomes.
- System prompt includes: State tier, surviving claims count, budget, rivalry standing, **State's credibility score** (not a personal metric)

### Lab Agent
- Generates radical hypotheses WITHOUT citing survivors
- Output labeled: HYPOTHESIS — UNVERIFIED
- Max 1 per State per cycle
- System prompt includes: open questions from Cities, recently destroyed claims, high-contradiction domains

---

## THE CLAIM PIPELINE (Constitution Article IV)

### Claim Types

**Foundation Claim** — extends existing proven knowledge:
```
Type: Foundation
Position: [specific enough to be wrong]
Archive Citations: [IDs — must be surviving or partial status]
Reasoning Chain:
  Step 1: From Claim #[ID] we know [X] because [reasoning]
  Step 2: [X] implies [Y] because [reasoning]
  Step 3: Combined with #[ID], this gives [Z] because [reasoning]
Therefore: [conclusion with boundaries]
```

**Discovery Claim** — genuinely new knowledge:
```
Type: Discovery
Position: [new proposal not in Archive]
First Principles:
  Axiom 1: [stated and justified]
  Axiom 2: [stated and justified]
Reasoning Chain:
  Step 1: From Axiom 1, derive [X] because [reasoning]
  Step 2: Combined with observation, gives [Y] because [reasoning]
Therefore: [conclusion]
Gap Addressed: [what's missing from Archive]
```

**Challenge Claim** — argues existing claim is wrong:
```
Type: Challenge
Target: Claim #[ID] — must be status "surviving"
What It Claims: [accurate restatement]
Where Wrong:
  Step [N]: assumes [X]
  [X] fails when [condition] because [reasoning]
Alternative: [what's actually true]
Evidence: [why alternative holds]
```

### Rival Pair Pipeline (Steps 1-17, per pair, per cycle)

```
1.  State A Researcher produces claim
2.  State B Researcher produces claim
3.  Engine validates both structurally
4.  Engine normalizes both (Haiku) → structured fields + keywords
5.  Engine decomposes both into premises (Haiku)
6.  State A Critic reads B's FULL claim + premises → challenges step
7.  State B Critic reads A's FULL claim + premises → challenges step
8.  State A Researcher rebuts B's Critic
9.  State B Researcher rebuts A's Critic
10. Engine checks rebuttal newness (Haiku)
11. Judge determines outcomes (Sonnet/Opus — domain-aware)
12. Full exchanges deposited in Archive
13. Scores assigned (drama/novelty/depth)
14. Stability scores updated for all surviving claims
15. Domain health computed
16. Content agents generate if thresholds met
17. Cycle log entries written
```

### Federal Lab Pipeline (Step 18, runs AFTER all rival pairs complete)

The Federal Lab is NOT part of the rival pair exchange. It is a separate track.

```
18. Federal Lab activates (if eligible this cycle):
    a. Select target claim from eligible domain (highest impact, rotation enforced)
    b. Decompose target's premises
    c. Invert one assumption → produce Challenge Claim
    d. Target State's Researcher writes rebuttal (separate from any rival rebuttal)
    e. Judge evaluates (same determine_outcome())
    f. Result deposited in Archive with full text
    g. Scores assigned, content evaluated

A State could face BOTH its rival's Critic AND the Federal Lab in the same cycle.
Both are independent exchanges with independent outcomes, independent scores,
independent Archive deposits.
```

### Step 19: End-of-Cycle Operations

```
19. After all pipelines complete:
    a. Check City auto-formation thresholds
    b. Check Town auto-formation thresholds
    c. Run abstraction pass if cycle % abstraction_interval == 0
    d. Check cross-domain bridges (keyword matching)
    e. Check tier advancement eligibility
    f. Check probation status for all States
    g. Write complete cycle_N.md log
    h. Update domain_health.json
    i. Update archive.md and archive.json
```

### Structural Validation

Foundation: position + 1 citation (surviving/partial) + 2 steps + conclusion. Citing destroyed/overturned = REJECTED.

Discovery: position + 1 first principle + 2 steps + conclusion + gap. **Reclassification check:** Haiku call sends position + top 5 surviving claims in domain → "Does this cover existing ground? {already_covered: bool, overlapping: [IDs]}." If covered → reclassify to Foundation, require citations.

Challenge: target ID (surviving) + restatement + flaw + alternative + evidence. Cannot challenge destroyed/overturned.

Failed validation → returned with errors, no token cost.

### Normalization (Haiku)
```json
{
  "claim_type": "foundation|discovery|challenge",
  "position": "core claim",
  "reasoning_chain": ["step 1", "step 2", "step 3"],
  "conclusion": "conclusion",
  "citations": ["#IDs"],
  "keywords": ["3-5 keywords for bridge detection"]
}
```

### Premise Decomposition (Haiku)
```json
{
  "explicit_premises": ["X", "Y", "Z"],
  "implicit_assumptions": ["A", "B"],
  "conclusion_depends_on": ["X", "Y", "Z", "A", "B"]
}
```
Critic receives BOTH full claim AND decomposition.

### Challenge Requirements
Must reference specific step. Vague challenges REJECTED by engine.

### Rebuttal Options
```
Option A — Defend: NEW reasoning not in original + new evidence
Option B — Concede and Narrow: specific concession + revised position
Option C — Retract: honest withdrawal + lessons learned
```

### Rebuttal Newness Check (Haiku)
```json
{"new_reasoning": true|false, "explanation": "why"}
```
If false → rebuttal is restatement → outcome determined by challenge strength.

### Option C Probation Rules
**Proactive retraction** (before challenge): +500 tokens, does NOT count toward probation.
**Option C under fire** (during challenge-rebuttal): +500 tokens, DOES count toward probation.
This prevents retraction farming.

### Anti-Loop Detection (Haiku)
Send State's last 3 claims to Haiku: "Do these use substantially the same citations, reasoning structure, and conclusion? {is_loop: bool, explanation: why}."
If loop → claim rejected. Same topic with new approach = allowed.

### Reasoning Depth Enforcement
Tier 0-1: 2 steps min | Tier 2: 3 | Tier 3: 4 | Tier 4+: 5

### Mandatory Dual Obligation
Every cycle: produce one claim AND Critic challenges rival. Both required.

### One Exchange Per Claim
Claim → Challenge → Rebuttal → Outcome → Done. No second rounds.

---

## THREE JUDGING SYSTEMS

### 1. Claim Outcome Judge — `determine_outcome()`

System-level LLM call. NOT an agent. No personality. Runs every cycle for every claim (rival and Federal).

**Input:** full claim + full challenge + full rebuttal + newness check + domain name + State approaches
**Output:**
```json
{
  "outcome": "survived|partial|retracted|destroyed",
  "reasoning": "detailed explanation",
  "open_questions": ["unresolved issues"],
  "scores": {"drama": 7, "novelty": 5, "depth": 8}
}
```

**Model:** Sonnet minimum. Opus if available. The judge is the quality gate — a weak judge produces a weak Archive. Do NOT use Haiku for judging.

**Critical judge prompt instructions:**
```
You are an expert evaluator with deep knowledge in [DOMAIN].

Evaluate the TRUTH and LOGICAL VALIDITY of the reasoning,
not just whether it is well-written.

For each reasoning step: verify that the logic actually holds
in this domain. If Step 3 claims X implies Y, check whether
X actually implies Y. Do not accept plausible-sounding
reasoning that is factually incorrect.

A well-written wrong argument is still WRONG.
A poorly written correct argument is still CORRECT.

If you are uncertain about domain-specific validity, say so
in open_questions. Do not guess. Flag uncertainty.

Apply your domain knowledge. If a physics claim violates
conservation of energy, it is wrong regardless of how
well-structured the argument is.
```

**Scoring Rubric (included in judge prompt):**
```
Drama:
  1-3: Routine exchange, no real tension
  4-6: Genuine disagreement, outcome uncertain mid-read
  7-8: Claim nearly destroyed or barely survived
  9-10: Paradigm confrontation, system-wide implications

Novelty:
  1-3: Incremental extension of existing knowledge
  4-6: New angle on known territory
  7-8: Genuinely new ground, no prior claims in area
  9-10: Cross-domain breakthrough or fundamental challenge

Depth:
  1-3: Surface reasoning, 2 steps, obvious conclusions
  4-6: Solid chain, 3-4 steps, some implicit work
  7-8: Deep reasoning, 5+ steps, hidden assumptions exposed
  9-10: Multi-layered argument with recursive dependencies
```

**Federal Lab logical consistency** is enforced by this same judge. Federal Lab claims go through determine_outcome() like any other claim. No separate validator needed.

Design as swappable: `JUDGE_MODEL = "claude"` → future: `"gpt"`, `"gemini"`, `"multi"`

### 2. Court (3 Judge Agents) — Appeals & Constitutional Only

**Originalist:** strict text interpretation of Constitution
**Pragmatist:** outcome-based interpretation
**Protectionist:** State sovereignty interpretation

Activated ONLY when:
- Senator files appeal (2,000 tokens from State budget)
- Senate action challenged for constitutionality
- Executive decision disputed

**Unanimous (3/3) to overturn. Split (2-1) upholds status quo.** All rulings and dissents archived.

The Court does NOT judge regular claims.

### 3. Founder Panels — Tier Advancement Only

Retired Founders stored as expertise profiles. 3 relevant Founders evaluate the State's PORTFOLIO:
- Tier 2: "Does this body of work show genuine argumentation depth?"
- Tier 3: "Do City analyses demonstrate real synthesis?"

Majority approval. LLM calls using stored profiles. No State token cost.

---

## FOUR OUTCOMES (Constitution Article IV Section 9)

**Survived:** Rebutted with new reasoning. Full claim stands. Full reward.
**Partial:** Conceded and narrowed. Valid portions deposited. Reduced reward.
**Retracted:** Withdrawn honestly. +500 integrity bonus. (Probation rules per Option C section.)
**Destroyed:** Couldn't rebut. Zero tokens. WHY it failed is recorded.

All four deposited in Archive with full exchange text.

---

## THE ARCHIVE (Constitution Article I)

### Entry Structure
```python
@dataclass
class ArchiveEntry:
    # Identity
    entry_id: str               # UUID
    display_id: str             # Global sequential "#001"
    entry_type: str             # claim | analysis | proposal | principle | governance_record
    source_state: str
    source_entity: str
    cycle_created: int
    status: str                 # surviving | partial | retracted | destroyed | founding
                                # | overturned | foundation_challenged | chain_broken

    # Claim details
    claim_type: str             # foundation | discovery | challenge
    position: str
    reasoning_chain: list
    conclusion: str
    keywords: list              # 3-5 for bridge detection

    # Full text (NEVER truncated)
    raw_claim_text: str
    raw_challenge_text: str
    raw_rebuttal_text: str
    lab_origin_text: str        # If from Lab hypothesis, else empty

    # Decomposition
    explicit_premises: list
    implicit_assumptions: list

    # Battle record
    challenge_step_targeted: str
    challenger_entity: str      # "Axiom Beta Critic" or "Federal Lab"
    outcome: str
    outcome_reasoning: str
    open_questions: list

    # Scores
    drama_score: int = 0
    novelty_score: int = 0
    depth_score: int = 0

    # Lineage
    citations: list = field(default_factory=list)
    referenced_by: list = field(default_factory=list)

    # Intelligence metrics
    stability_score: int = 1    # Starts at 1. +1 per cycle survived, +1 per challenge survived
    impact_score: int = 0       # Downstream dependency count (computed)

    # Economy
    tokens_earned: int = 0
```

### Archive Rules
- Append-only. Text never modified. Status can change.
- `referenced_by` updated dynamically when new entries cite existing ones.
- All agents have full read access.

### Claim Graph
Archive IS a directed graph. `citations` = edges up, `referenced_by` = edges down.

Queries: what depends on #047? What collapses if #047 overturned? Deepest chain? Highest impact?

### Chain Collapse (recursive, ALL entry types)

When a claim is overturned:
1. Claim status → "overturned" (text preserved)
2. **Other claims** citing ONLY that claim → "foundation_challenged"
3. Claims where overturned entry is minority of citations → STAND with note
4. **City analyses** where >50% of citations overturned → "foundation_challenged"
5. City analyses where minority overturned → stand with note
6. **Town proposals** on flagged analyses → "chain_broken"
7. **Recursive:** walk full dependency tree. If #005 overturned → #020 flagged → #035 (cites only #020) also flagged.
8. **Max depth: 10 levels.** If recursion exceeds 10, log "deep collapse detected — manual review recommended" and stop.
9. All flagged items stay in Archive, excluded from tier calculations.
10. Collapse logged in cycle_N.md and archive.md.

### Credibility Score
```
credibility = surviving_claims / total_pipeline_claims
```
**Denominator includes:** survived, partial, destroyed, retracted — all claims that entered the adversarial pipeline.
**Excludes:** founding deposits, failed validation (never entered pipeline), City analyses, Town proposals.
Retracted = neutral in denominator (submitted but not failed).

---

## FEDERAL RESEARCH AGENCY

One independent agent. Not attached to any State. **Budget-free** (system agent, no token costs).

Activates at cycle 5 OR 10+ surviving claims (whichever first).

**Cannot target same domain two cycles in a row.** If only one eligible domain exists, **skip that cycle** and log "Federal Lab: no eligible domain (rotation cooldown)." Next cycle cooldown resets.

Each cycle after activation:
1. Identify highest-impact claim in eligible domain
2. Decompose premises
3. Invert ONE assumption
4. Submit as Challenge Claim
5. Target State's Researcher rebuts
6. Judge evaluates via determine_outcome()
7. Full exchange deposited

Crazy allowed. Logical inconsistency enforced by the judge automatically (no separate validator).

---

## ABSTRACTION AND COMPRESSION

Every 5 cycles, per domain: top 20 highest-impact surviving claims (**per-domain cap**, not global — 6 domains = 6 separate calls).

Principles deposited as entry_type "principle." **Challengeable starting the NEXT cycle** — not the cycle they're created. They cite the claims they compress.

### Metric Definitions

**compression_ratio** = `principle_entries / surviving_claims` per domain. 0 until first abstraction pass. Higher = more abstraction achieved.

**lab_survival_rate** = `lab_origin_claims_survived / total_lab_origin_claims` per domain, all-time (not rolling). A claim counts as lab origin if `lab_origin_text` is populated.

### Contradiction Tracking
Identified, resolved, defended, net per cycle.

---

## TIER SYSTEM (Constitution Article V)

```python
V2_TIERS = {
    0: {"name": "Empty", "surviving_claims": 0},
    1: {"name": "Foundation", "surviving_claims": 5},
    2: {"name": "Argumentation", "surviving_claims": 15,
        "validation": "Founder panel (stored profiles, LLM call)"},
    3: {"name": "Depth", "surviving_claims": 30,
        "requires": "1+ active City"},
    4: {"name": "Application", "surviving_claims": 50,
        "requires": "1+ active Town"},
    5: {"name": "Influence", "surviving_claims": 75,
        "requires": "10+ entries from OTHER domains cite THIS State's claims"}
}
```

**Tier 5 direction:** Being cited (influence), not citing others (breadth).

**Active City:** has produced 1+ published analysis AND parent State not dissolved.
**Active Town:** has produced 1+ published proposal AND parent State not dissolved.

---

## TOKEN ECONOMY (Constitution Article VI)

Initial: 30,000 (test) / 50,000 (production). Cycle cost: 2,000 (test) / 3,000 (production).

**NO lifelines. States die. Balance floors at zero (clamped).**

### Earnings
| Event | Tokens |
|-------|--------|
| Foundation Survived | +2,000 |
| Foundation Partial | +1,200 |
| Discovery Survived | +1,000 (→ +3,000 when first cited — **permanent**, not clawed back if citing claim later overturned) |
| Discovery Partial | +600 |
| Challenge Succeeded | +4,000 |
| Challenge Failed | -1,000 (clamped to 0 floor) |
| Retracted (any) | +500 |
| Destroyed | +0 |
| Rival destroyed by your Critic | +1,000 |
| Rival narrowed by your Critic | +800 |
| City published | +1,000 |
| City cited by Town | +1,500 |
| City cross-domain cited | +2,000 |
| Town published | +500 |
| Town human-accepted | +5,000 **(future — requires human review interface)** |
| Town cross-domain cited | +3,000 |
| Tier advancement | +10,000 |
| Cross-domain citation | +1,500 |
| Domain milestone (25/50/100) | +2,000 to WEAKER rival |

---

## CITIES (Constitution Article XII)

**Auto-formation:** 5+ surviving claims from SAME State sharing 2+ citations in common = cluster → City spawns. **State-bound.** Rival States can independently form Cities on overlapping clusters — their analyses differ due to methodological approaches. That's valuable.

One agent: **Analyst.** Structural validation, not adversarial.

### Cities Spawn Research Directions
Open questions flagged as "Research Direction" in Archive. Visible to all Labs.

---

## TOWNS (Constitution Article XIII)

**Auto-formation:** 3+ published analyses from ANY Cities within the same State → Town spawns.

One agent: **Builder.** Full citation chain enforced (Claim → Analysis → Proposal). Town output IS the product.

---

## ANTI-LOOP PROTOCOL

Haiku LLM call on State's last 3 claims. Checks same citations + same structure + same conclusion. All three match → rejected. Same topic, new approach → allowed.

---

## GOVERNANCE

### Senate
- 1 Senator per active State. Dissolved States lose seat.
- **Minimum quorum: 3 Senators for any valid vote.** If fewer than 3 active States, Senate suspended. No votes, no amendments, no dissolution hearings. Claim pipeline continues — governance pauses until replacement States restore quorum.

### Executive
- First term: 2 candidates (Synthesis + Challenger). Subsequent: 3 (+ Incumbent).
- Every 10 cycles. Controls budgets, thresholds, bonuses.
- Parameters locked for full term. Removal: 2/3 Senate or unanimous Court.

### Court
- 3 Judges, fixed philosophies. Unanimous to overturn.
- Appeals and constitutional disputes ONLY. Does not judge claims.

### Amendments
- 2/3 Senate + Court review. 5 cycle cooling. Non-amendable clauses permanent.

---

## INTELLIGENCE GROWTH MECHANISMS

### Stability Score
`1 (base on deposit) + cycles_survived + independent_challenges_survived`

### Meta-Learning
Researcher sees last 3-5 destroyed claims with judge reasoning before producing new claims.

### Credibility Score
`surviving / total pipeline claims` per State. < 0.3 → Federal targeting increases. < 0.2 → Senate notified.

### Frontier Pressure
Stable domains get more Federal destabilization.

### Cross-Domain Bridges
Keywords extracted during normalization. 2+ shared keywords across domains = bridge. No embeddings.

---

## DOMAIN MATURITY INDEX

Per domain per cycle. Single `domain_health.json` file with all domains as top-level keys.

```json
{
  "Mathematics": {
    "surviving_claims": 28,
    "total_claims": 45,
    "survival_rate": 0.62,
    "credibility_a": 0.65,
    "credibility_b": 0.58,
    "compression_ratio": 0.35,
    "contradiction_trend": "decreasing",
    "avg_dependency_depth": 4.2,
    "max_chain_length": 7,
    "cross_domain_citations": 5,
    "active_cities": 2,
    "active_towns": 1,
    "lab_survival_rate": 0.30,
    "federal_challenges_survived": 3,
    "maturity_phase": "Structured Abstraction"
  }
}
```

### Phase Thresholds
```
Volatile Exploration:    survival_rate < 0.3 OR contradiction_trend == "increasing"
Stabilizing Foundation:  survival_rate 0.3-0.5 AND contradiction_trend != "increasing"
Structured Abstraction:  survival_rate > 0.5 AND compression_ratio > 0.2 AND active_cities > 0
Applied Integration:     active_towns > 0 AND cross_domain_citations > 3
Mature Influence:        cross_domain_citations > 10 AND survival_rate > 0.6
```

### Intelligence Growth = Five Metrics Trending:
1. Survival rate ↑  2. Compression ratio ↑  3. Contradictions ↓
4. Dependency depth ↑  5. Cross-domain citations ↑

---

## CONTENT SYSTEM

### Four Formats

**Blog Post (500-1000 words):** Tier advancement, dissolution, dramatic claim destroyed, amendment. Science journalist. Rolling context in `blog_context.json` — max 20 entries, 200 words each. Oldest dropped when exceeds 20.

**Newsroom Clip (150-200 words):** drama >= 7, any destroyed, any partial concession. Breaking news, hook-driven.

**Live Debate Feed (60-90s TikTok):** drama >= 8 OR novelty >= 8. Sports commentary play-by-play.

**Explorer Log (200-300 words):** New State/City/Town, milestones, dissolution ("ruins"). First-person travel blog. Worldbuilds from data.

### Selection
| Condition | Content |
|-----------|---------|
| drama >= 8 AND depth >= 7 | ALL FOUR |
| drama >= 7 OR novelty >= 8 | Newsroom + Debate |
| drama >= 5 AND (tier/destroyed) | Blog |
| New State/City/Town/milestone | Explorer |
| Dissolution | ALL FOUR (drama = 10 automatic) |
| drama < 5 AND routine | SKIP |

### Content Input
FULL exchange text (never truncated): claim, challenge, rebuttal, outcome, scores, context.

---

## MODEL ALLOCATION

| Task | Model | Rationale |
|------|-------|-----------|
| Normalization | Haiku | Structured extraction, cheap |
| Premise decomposition | Haiku | Structured extraction |
| Rebuttal newness check | Haiku | Binary comparison |
| Anti-loop detection | Haiku | Pattern matching |
| Discovery reclassification | Haiku | Coverage check |
| Bridge keyword extraction | Haiku | Simple extraction |
| Content generation | Haiku | Creative but not critical |
| Researcher claims | Sonnet | Core reasoning quality |
| Critic challenges | Sonnet | Core attack quality |
| Researcher rebuttals | Sonnet | Core defense quality |
| **determine_outcome() judge** | **Sonnet/Opus** | **Quality gate — strongest available** |
| Court judges (appeals) | Sonnet | Governance reasoning |
| Founder panels | Sonnet | Portfolio evaluation |
| Federal Lab hypothesis | Sonnet | Creative disruption |

---

## FILES TO CHANGE

| File | Changes |
|------|---------|
| **`core/models.py` (NEW)** | ModelRouter class. `get_model(task_type)` returns appropriate client. Maps task types to Haiku/Sonnet/Opus per allocation table. Single config point for model swapping. |
| `governance/states.py` | State (4 agents, no Governor), ArchiveEntry, claim pipeline, credibility, stability, token clamping |
| `governance/perpetual.py` | Rival pair pipeline (steps 1-17), Federal Lab pipeline (step 18), end-of-cycle ops (step 19), content selection, cycle logs, domain health, abstraction, bridges |
| `core/engine.py` | Delete convention/Jefferson, load Constitution, V1 check (+ optional --force-clean), fresh start, archive export |
| `core/persistence.py` | ArchiveEntry schema, graph queries, display IDs (global counter), domain health, chain collapse (recursive, max depth 10) |
| `config/settings.py` | MOCK_CONFIG, V2_TIERS, JUDGE_MODEL, token values, model allocation map |
| `content/generator.py` | Four agents, score selection, blog rolling context (capped), Haiku |
| `agents/base.py` | Researcher/Critic/Senator/Lab configs, Founder profiles, Court judges, Federal Lab agent |
| `founders/convention.py` | Phase 0 research only, delete all drafting logic |

---

## SUCCESS CRITERIA (36 checks)

1. ✅ Constitution loaded from file (no agent drafting)
2. ✅ Rival pairs formed (2 per domain)
3. ✅ Cross-challenge (Critic attacks rival, never self)
4. ✅ Structured claims (Foundation/Discovery/Challenge)
5. ✅ Free-form Phase 0 deposits (no structural validation)
6. ✅ Premise decomposition before Critic sees claim
7. ✅ Rebuttal newness check enforced
8. ✅ Option C under fire counts toward probation
9. ✅ Outcomes (survived/partial/retracted/destroyed)
10. ✅ Domain-aware judge with scoring rubric (Sonnet+)
11. ✅ Court for appeals only (3 judges, unanimous to overturn)
12. ✅ Founder panels for tier advancement only (stored profiles)
13. ✅ Full text in Archive (NEVER truncated)
14. ✅ Global display IDs (#001 unique across system)
15. ✅ archive.md human-readable with full battle records
16. ✅ cycle_N.md per cycle with domain health
17. ✅ domain_health.json with phase thresholds and metric definitions
18. ✅ Content based on score thresholds
19. ✅ Four content formats, distinct personalities
20. ✅ Blog Writer rolling context (max 20 entries)
21. ✅ Lab hypotheses via Researcher choice (→ Discovery Claim)
22. ✅ Federal Lab: budget-free, rotation, activation delay, skip if no eligible domain
23. ✅ Federal Lab pipeline separate from rival pair pipeline (step 18)
24. ✅ Claim graph (citations, referenced_by)
25. ✅ Chain collapse recursive (max depth 10, all entry types, >50% threshold)
26. ✅ Abstraction pass (20 per domain, principles challengeable next cycle)
27. ✅ Cross-domain bridges (keyword matching, no embeddings)
28. ✅ Token economy (budget, earnings, dissolution, balance floors at 0)
29. ✅ States die — all content on dissolution
30. ✅ Stability score (starts at 1)
31. ✅ Credibility score (pipeline claims only, excludes founding)
32. ✅ Meta-learning (failure history in Researcher prompt)
33. ✅ Anti-loop via Haiku (patterns not topics)
34. ✅ Warmup: both States skip challenge for 3 cycles
35. ✅ First election: 2 candidates only
36. ✅ V1 data: refuse to start (optional --force-clean flag)
37. ✅ ModelRouter in core/models.py (multi-model architecture)
38. ✅ Senate quorum minimum: 3 Senators
39. ✅ compression_ratio and lab_survival_rate defined
40. ✅ Output directory structure defined
